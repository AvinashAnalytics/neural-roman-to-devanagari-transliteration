# ===============================================================
# CONFIGURATION FOR HINDI TRANSLITERATION (CS772 Assignment 2)
# ===============================================================

data:
  language: hin
  data_url: "https://huggingface.co/datasets/ai4bharat/Aksharantar/resolve/main"
  max_train_samples: 100000
  # ⚠️ Your data uses .json extension (JSONL format), not .csv.jsonl
  train_file: "hin_train.json"
  valid_file: "hin_valid.json"
  test_file: "hin_test.json"

# ===============================
# LLM SETTINGS (Optional)
# ===============================
llm:
  max_tokens: 100
  models:
    anthropic:
      - claude-3-haiku-20240307
      - claude-3-sonnet-20240229
    deepinfra:
      - meta-llama/Llama-2-70b-chat-hf
      - mistralai/Mixtral-8x7B-Instruct-v0.1
    google:
      - gemini-pro
    groq:
      - llama3-70b-8192
      - mixtral-8x7b-32768
      - gemma-7b-it
    openai:
      - gpt-3.5-turbo
      - gpt-4-turbo
  # Temperature controls randomness (0 = deterministic)
  temperature_values:
    - 0.1
    - 0.3
    - 0.5
    - 0.7
    - 0.9
    - 1.0
  # top_p controls nucleus sampling
  top_p_values:
    - 0.9
    - 0.95
    - 0.99
    - 1.0

# ===============================
# LSTM SETTINGS
# ===============================
lstm:
  bidirectional: true         # Use bidirectional LSTM
  dropout: 0.3                # Dropout for regularization
  embedding_dim: 256          # Character embedding dimension
  hidden_dim: 512             # LSTM hidden size
  num_layers: 2               # Maximum 2 layers for assignment

# ===============================
# TRANSFORMER SETTINGS
# ===============================
transformer:
  d_model: 256                     # Model dimension
  n_heads: 8                       # Number of attention heads
  num_layers: 2                    # Maximum 2 layers
  d_ff: 1024                       # Feed-forward hidden dimension
  dropout: 0.1                     # Dropout for regularization
  max_seq_length: 100              # Maximum sequence length
  use_local_attention: true        # Use local attention instead of standard
  local_attention_window: 5        # Local attention window size

# ===============================
# TRAINING HYPERPARAMETERS
# ===============================
training:
  batch_size: 128                  # Number of examples per batch
  epochs: 1                       # Total epochs (set 3 for quick demo; adjust if needed)
  learning_rate: 0.0005           # Adam optimizer learning rate (lower for Transformer stability)
  gradient_clip: 1.0              # Clip gradients to prevent explosion (lower for stability)
  teacher_forcing_ratio: 0.5      # Ratio for LSTM training
  # Beam sizes for evaluation: greedy = 1, beam search = 3,5,10
  beam_sizes:
    - 1
    - 3
    - 5
    - 10

# ===============================================================
# END OF CONFIG
# ===============================================================